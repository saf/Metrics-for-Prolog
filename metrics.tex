\documentclass[11pt,a4paper,twoside]{article}
\usepackage{a4wide}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{multicol}

\RequirePackage{times}
\RequirePackage{a4wide}
\RequirePackage{longtable}
\RequirePackage{multicol}
\RequirePackage{url}

\newcommand{\pname}{\emph{MFPL}}

\author{SÅ‚awomir Rudnicki}
\title{Adapting code metrics to Prolog}
\date{}

\begin{document}
\maketitle
\begin{abstract}
  Little effort has been put into developing methods for measuring
  quantitative values of complexity and maintainability of logic-based
  programs. This work presents the approach towards adapting known
  software metrics to Prolog code, as well as \emph{Metrics for
    Prolog}, a simple Prolog metrics tool.
\end{abstract}
\begin{multicols}{2}

\section{Introduction}

In the software industry, there is a need to control the complexity
and maintainability of the produced code. The most well-known
approaches to measuring this maintainability, which provide
quantitative results, were introduced in the late 1970s by Thomas
McCabe Sr. \cite{mccabe}, who based his metric on computational
features of programs, and Maurice Halstead, who in turn proposed
metrics that relied on psychological aspects of understanding software
code.

There has not been much effort put into measuring the complexity of
code when it comes to declarative and logic-based programming. I have
developed a simple tool, called \emph{Metrics for Prolog} or
\emph{MFPL}, that provides some simple metrics for code written in
Prolog, a logic-based programming language. 

This document presents a discussion of whether and how some software
metrics may be adapted to modern Prolog code.

\section{The Prolog language}

Prolog is a declarative programming language, whose syntax is based on
first-order logic. Since its creation around the year 1972, it has
found multiple applications in the fields of artificial intelligence
and natural language processing.

Prolog programs consist of definitions of \emph{relations} on
\emph{terms}. Intuitively, a run of the program seeks to determine
whether a relation holds for the given arguments, or to find a set of
arguments such that a given relation holds. In this document, I will
briefly describe the basics of logic programming to the extent that is
needed to understand the problems of adapting code metrics to it. A
thorough introduction to the core syntax and semantics of Prolog
itself can be found in the classic book by Clocksin and Mellish
\cite{clocksin}.

\section{Local complexity measure}

An early approach towards measuring code metrics of Prolog has been
presented in a 1985 work by Markusz and Kaposi\cite{markusz}. They
proposed a metric designed specifically for Prolog, which is simple to
compute and gives a view on the complexity of given clauses (or
\emph{partitions}). The metric, which will here be called the
\emph{local complexity measure}, or LC, allows for measuring both the
complexity level and the size of the whole file or program.

In MFPL, I have implemented a modified version of what Markusz and
Kaposi describe as their first attempt to their complexity measure. 
It is defined as:
$$ C = P_1 + P_2 + P_3 + P_4, $$
where:
\begin{itemize}
\item $P_1$ is the number of new data entities in the left hand side
  of the partition. I have interpreted this as the number of new
  non-trivial variables introduced in the positive atom.
\item $P_2$ is the number of subproblems the clause divides the
  problem into. This is interpreted as the number of atoms in the
  right hand side of the clause.
\item $P_3$ measures the complexity of relations between the
  subproblems. Markusz and Kaposi suggested that $2$ should be added
  here for every recursive call of the defined predicate. Since the
  set of Prolog operators that change the relationships between the
  subproblems, I have decided to modify this approach by also adding
  $1$ for each disjunction (introduced by the operator \texttt{;}) and
  implication (introduced by operators \texttt{->} and
  \texttt{*->}).
\item $P_4$ is the number of variables introduced in the right hand
  side of the clause.
\end{itemize}
However in the original work the measure was only applied to clauses, 
in MFPL it is also used to measure facts and commands. 

\section{Halstead's metrics}

Halstead \cite{halstead} proposed metrics for analysing the effort one
needs to make to code a program and then maintain it. The metrics are
largely based on psychological research of how one understands a
program by making ``mental comparisons'' of tokens.

Halstead's metrics are defined using two groups of source code tokens:
\emph{operators} and \emph{operands}. Let us denote:
\begin{itemize}
\item $N_1$ -- the number of all operators, 
\item $N_2$ -- the number of all operands, 
\item $n_1$ -- the number of distinct operators, 
\item $n_2$ -- the number of distinct operands.
\end{itemize}

The following metrics are then derived from those counts:
\begin{description}
\item[Program length]
  $$N = N_1 + N_2,$$ denotes the total length of the program, measured
  as the number of all tokens.
\item[Vocabulary]
  $$n = n_1 + n_2,$$ denotes the size of the program's vocabulary,
  i.e.  the number of distinct tokens that one has to consider to
  fully understand the program.
\item[Volume]
  $$V = N \cdot \log_2n,$$ which measures the size of the program when
  coded in binary form. The metric is based on the fact that every
  token of the analysed program can be coded with $log_2n$ bits.
\item[Difficulty]
  $$D = \frac{n_1}{2}\cdot\frac{N_2}{n_2},$$ measures how difficult
  the program is to create and maintain. According to Halstead, code
  is more sophisticated and error-prone if distinct operands occur
  many times in it..
\item[Level]
  $$L = \frac{1}{D}.$$ The inverse of difficulty, this metric may
  indicate whether a given program is written on a high- or
  low-level. For example, assembler code will tend to have a lower
  level value than Java code.
\item[Effort]
  $$E = V \cdot D.$$ The effort needed to write or understand a
  program is proportional to its size and difficulty.
\end{description}

Halstead moves on to propose experimentally derived, higher-level
metrics of code, including:
\begin{description}
\item[Time]
$$T = E/18,$$ which is the time needed to ``mentally'' implement the
  program, in seconds.
\item[Bugs]
$$B = E^{\frac{2}{3}}/3000,$$ which is the number of bugs that are
  delivered, on average, inside the program.
\end{description}

\subsection{Operators and operands}
The main problem with Halstead's metrics, however, is the definition
of what is an operator and what is an operand. For popular languages,
those notions have been thoroughly discussed. Nandy \cite{nandy} sums
up what seems to be the most common understanding of what operators
and operands are in C, C++ and Java. 

In declarative programming, it is not obvious how to discern operators
from operands. All language constructs of Prolog are in fact terms of
the form:
$$T(x_1, x_2, \dots, x_k).$$ 
Here, the number $k$ is the \emph{arity}
of the predicate symbol $T$, and $x_1, \dots, x_k$ are its
\emph{arguments}. We usually write $T/k$ to denote a term and its arity. 

When we thus write:
$$A\,\, is\,\, B + 1,$$ 
Prolog understands it as nothing but:
$$is(A, +(B, 1)).$$ When one takes into account the fact that function
names in Java function calls are commonly considered to be operands,
it is therefore tempting to view all subterms of a term as operands.

To add to this, Prolog is often used to write meta-programs --
programs that operate on Prolog source code. Some programs may even
manipulate parts of themselves during normal operation, further
blurring the notions of operators and operands. Of course,
\pname\ itself is an example of a meta-program.

In \pname, I have chosen a simple way of dividing tokens into operands
and operators. In a few words, it can be described as \emph{If it looks
  like an operator, it is an operator}. Operators are therefore all
symbols that may be used without the parenthetical notation $T(x, y)$
and may be instead written as: 
\begin{itemize}
\item $T\, x\, y$ (prefix notation),
\item $x\,T\, y$ (infix notation), or 
\item $x\, y\, T$ (postfix or reverse Polish notation).
\end{itemize}
All other relational symbols will be viewed as operands, even if they
are never an argument for another symbol.

The above decision is one that seems to be most in line with the
psychological basis of Halstead's metrics. When reading a Prolog
program, or any other code, we naturally consider tokens that take
advantage of the infix notation as operators, because this is what we
are used to from studying mathematics. Hovewer informal, this
observation would certainly be more important from Halstead's point of
view than the actual internal representation of terms.

We will therefore consider all built-in Prolog operators to be
operators in terms of Halstead's metrics. The built-in operators are,
for example:
\begin{itemize}
\item the rule-building predicate :-$/2$,
\item the conjunction and disjunction operators $,/2$ and $;/2$,
\item arithmetic operators, including $is/2$, $+/2$, $*/2$, 
\item arithmetic relations, such as $=</2$, 
\item bitwise and logical operators, including the logical alternative
  operator $\//2$ and the bitwise shift operator $<</2$.
\end{itemize}

ISO-Prolog contains a mechanism that allows the user to define their
own operators using the built-in predicate $op/3$. By writing: 
\begin{center}
\texttt{op(400, fx, '***')},
\end{center}
 we can allow the symbol \texttt{***}$/1$ to be used in prefix
notation. \pname's Halstead metrics will treat all such tokens as
operators.

There are also a few purely syntactical tokens that will also be
treated as operators:
\begin{itemize}
\item the pair of parentheses $()$ surrounding a symbol's arguments
  will be viewed as a single operator,
\item every comma between arguments of a term will be viewed as an
  operator.
\item the dot that stands after every Prolog term will be viewed as a
  single operator, by analogy to C's semi-colon,
\item the list notation \texttt{[H | T]} will be treated as one
  operator.
\end{itemize}

\subsection{Variable distinctiveness}

Halstead's metrics heavily rely on which operands and operators we
consider to be \emph{the same}. Whereas this is no problem with
operators, atoms and integers, it becomes problematic when it comes to
variables. In MFPL, I decided to make the following assumptions:
\begin{itemize}
\item Variables occurring in different clauses of the same predicate
  are considered distinct, even if they have the same name. This is
  because the variables do not share a common scope (as we would call
  it if Prolog were a procedural language). 
\item Singleton variables (i.e. variables that occur only once in a
  clause) are all considered distinct, even though they are often
  denoted by the same name ``\_''. This assumption is again derived
  from the psychological basis of Halstead's metrics: even though the
  programmer does not have to ``mentally compare'' the singleton
  variable to other variables, he has to take note of its position as
  an argument of a term and only then can they consider it irrelevant.
\end{itemize}

\subsection{Other metrics}

Logic-based programming strongly differs from imperative programming
in that the programmer states a problem and lets the built-in
inference mechanism find the solution instead of providing means to
solve the problem. Thus, not every software metric can be sensibly
adapted to Prolog. For example, the cyclomatic complexity (CC) metric
introduced by Thomas J. McCabe\cite{mccabe}, however general, can only
apply to procedural languages. Without going into the details, CC is
equal to the minimum number of execution paths that span the whole
execution tree, in which branching is introduced by checking a logical
condition. 

Due to the declarative nature of Prolog, not every language construct
can be interpretted as a part of the execution tree in a
straightforward way. As a simple example, let's take the following
one-clause predicate:
\begin{center}
\texttt{succ(A, B) :- B is A+1.}
\end{center}
When we start to think in procedural terms, as suggested by McCabe's
metric, calling this predicate may have two different outcomes. On one
hand, it may act as a simple assignment statement, equivalent to
\texttt{B~:=~A+1}. On the other hand, it may be a logical check,
equivalent to \texttt{B~==~A+1} and would as such introduce a branch
in the execution tree. 

Such problems multiply when more complex language constructs are
considered. Therefore, I did not attempt to include McCabe's metric,
or other ``procedural'' metrics, in MFPL.

\end{multicols}

\newpage

\begin{thebibliography}{halstead}
\bibitem{clocksin}
  W.F. Clocksin, C.S. Mellish, 
  \emph{Programming in Prolog}, 
  Springer-Verlag Telos, 1994.
\bibitem{markusz}
  Z. Markusz and A.A. Kaposi 
  \emph{Complexity Control in Logic-based Programming},
  The Computer Journal, 28: pp. 487-495, 1985.
\bibitem{halstead}
  M.H. Halstead, 
  \emph{Elements of Software Science}, 
  Elsevier, North-Holland, New York, 1977.
\bibitem{nandy}
  I. Nandy, 
  \emph{Halstead's Operators and Operands}
  Scribd.com, 2007.
\bibitem{mccabe}
  T.J. McCabe, 
  \emph{A Complexity Measure}, 
  IEEE Transactions on Software Engineering: pp. 308â€“320, 1976.
\end{thebibliography}
\end{document}
